# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# LangSmith Configuration (Optional but recommended for monitoring)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=multi-agent-system

# Local LLM Configuration (Ollama)
OLLAMA_API_BASE=http://localhost:11434
OLLAMA_MODEL=phi4:latest
OLLAMA_VISION_MODEL=moondream:latest

# Hugging Face Configuration
HUGGINGFACE_API_TOKEN=your_hf_token_here
HF_MODEL_NAME=microsoft/Phi-4
HF_VISION_MODEL=vikhyatk/moondream2
HF_DEVICE=auto  # Options: auto, cuda, cpu
HF_CACHE_DIR=./models  # Local model cache directory
USE_QUANTIZATION=true  # Enable 4-bit quantization for memory efficiency

# Model Provider Selection
MODEL_PROVIDER=ollama  # Options: ollama, huggingface, openai
FALLBACK_PROVIDER=huggingface  # Fallback if primary provider fails

# Alternative Local LLM Endpoints
# LOCALAI_API_BASE=http://localhost:8080
# OPENLLM_API_BASE=http://localhost:3000

# Document Intelligence Configuration
DOC_LOADER_TYPE=docling  # Options: docling, markitdown
MAX_CONTEXT_TOKENS=8192  # For models with limited context
ENABLE_PII_MASKING=true
DOCUMENT_UPLOAD_PATH=./documents
EXTRACTED_DATA_PATH=./extracted

# Application Configuration
DEBUG=false
LOG_LEVEL=INFO

# Model Configuration (Optional - will use defaults if not set)
DEFAULT_MODEL=gpt-4-turbo-preview
TEMPERATURE=0.7
MAX_TOKENS=2000
